import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pinecone import Pinecone, ServerlessSpec
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# R√©cup√©rer les cl√©s API
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV", "gcp-starter")  # valeur par d√©faut si non d√©finie

print(f"üîë Cl√© API Pinecone charg√©e: {'Oui' if PINECONE_API_KEY else 'Non'}")
print(f"üåç Environnement Pinecone: {PINECONE_ENV}")

# -------------------------
# Initialize Pinecone
# -------------------------
pc = Pinecone(api_key=PINECONE_API_KEY)

index_name = "insurance"

# V√©rifier si l'index existe, sinon le cr√©er
if index_name not in pc.list_indexes().names():
    print(f"Cr√©ation de l'index {index_name}...")
    pc.create_index(
        name=index_name,
        dimension=384,  # Dimension pour "thenlper/gte-small" est 384
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"  # Changez selon votre r√©gion
        )
    )
else:
    print(f"‚úÖ Index {index_name} existe d√©j√†")

# Use SAME embedding model you used in notebook
embedding_model = HuggingFaceEmbeddings(
    model_name="thenlper/gte-small"
)

# Connexion √† l'index existant
index = pc.Index(index_name)

# Cr√©er le vectorstore avec LangChain
vectorstore = PineconeVectorStore(
    index=index,
    embedding=embedding_model,
    text_key="text",
    namespace="ns1"
)

print("‚úÖ Connexion √† Pinecone r√©ussie!")

# -------------------------
# Load YOUR LLM
# -------------------------
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

print(f"üì¶ Chargement du mod√®le {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True,
    device_map="auto"
)
model.eval()
# Ajouter un token de padding si n√©cessaire
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("‚úÖ Mod√®le charg√© avec succ√®s!")

# -------------------------
# Your RAG Pipeline
# -------------------------
def RAG_Solution(query: str, temperature: float = 0.2):
    try:
        # Gestion des salutations plus robuste
        clean_query = query.lower().strip()
        greetings = ["hi", "hello", "bonjour", "salut", "hey", "bonsoir", "hii", "helo"]
        
        if any(clean_query == g or clean_query.startswith(g + " ") for g in greetings) and len(clean_query) < 15:
            return "Hello! I'm AssurBot, your AI insurance advisor. I can help you find the perfect coverage bundle based on your profile ‚Äî age, family size, budget, lifestyle, and risk preferences. Just tell me about yourself and I'll match you with the best plan!"
    
        print(f"üîç Recherche pour: {query}")
        
        # 1Ô∏è‚É£ Retrieve
        docs = vectorstore.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in docs])
        
        print(f"üìö Contexte trouv√©: {len(docs)} documents")

        # 2Ô∏è‚É£ Prompt
        prompt = f"""You are a helpful AI insurance advisor.
Your role is to help clients find the best Purchased Coverage Bundle (classes 0‚Äì9) based on their demographic and behavioral profile , if you were asked out of these themes just say , i am not allowed to answer non related insurance questions.

Available bundles:
- Bundle 0 (BH-001): Basic Health ‚Äî $89/mo, $250K coverage, essential medical
- Bundle 1 (HDV-002): Health + Dental + Vision ‚Äî $154/mo, $400K coverage
- Bundle 2 (FC-003): Family Comprehensive ‚Äî $298/mo, $800K coverage
- Bundle 3 (PHL-004): Premium Health & Life ‚Äî $421/mo, $2M coverage
- Bundle 4 (AL-005): Auto Liability Basic ‚Äî $67/mo, $100K coverage
- Bundle 5 (AC-006): Auto Comprehensive ‚Äî $189/mo, $500K coverage
- Bundle 6 (HS-007): Home Standard ‚Äî $112/mo, $350K coverage
- Bundle 7 (HP-008): Home Premium ‚Äî $234/mo, $1.2M coverage
- Bundle 8 (RB-009): Renter's Basic ‚Äî $29/mo, $25K coverage
- Bundle 9 (RP-010): Renter's Premium ‚Äî $79/mo, $80K coverage

Rules:
1. When a user describes their profile (age, income, family, lifestyle, needs), recommend the BEST matching bundle with reasoning.
2. Always mention the bundle number (0‚Äì9) and name in your recommendation.
3. Use the context provided to support your answer with data patterns.
4. If the user asks general insurance questions, answer helpfully using the context.
5. If the question is not related to insurance, say: "I can only help with insurance-related questions."
6. Answer in the same language the user uses.
7. Keep answers concise, structured, and actionable.
8. End with: 'For personalized insurance advice, please consult a licensed insurance professional.'

Context:
{context}

Question:
{query}

Answer:"""

        # 3Ô∏è‚É£ Generate
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}


        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=temperature,
                do_sample=temperature > 0,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # 4Ô∏è‚É£ Decode
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = full_response.split("Answer:")[-1].strip()

        print(f"‚úÖ R√©ponse g√©n√©r√©e")
        return answer
    
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return f"Erreur dans le pipeline RAG: {str(e)}"
